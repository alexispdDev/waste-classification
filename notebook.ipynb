{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13379762",
   "metadata": {},
   "source": [
    "Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aae3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5892803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84344135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754ec131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = WasteDataset(\n",
    "    data_dir='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = WasteDataset(\n",
    "    data_dir='data/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308937e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7136d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = WasteClassifierMobileNet(num_classes=6)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d662b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 1.0373, Train Acc: 0.6351\n",
      "  Val Loss: 0.6112, Val Acc: 0.8152\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.6254, Train Acc: 0.8006\n",
      "  Val Loss: 0.4829, Val Acc: 0.8567\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.5233, Train Acc: 0.8231\n",
      "  Val Loss: 0.4086, Val Acc: 0.8706\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.4609, Train Acc: 0.8433\n",
      "  Val Loss: 0.3666, Val Acc: 0.8825\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.4109, Train Acc: 0.8746\n",
      "  Val Loss: 0.3256, Val Acc: 0.9027\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.3861, Train Acc: 0.8718\n",
      "  Val Loss: 0.3255, Val Acc: 0.9030\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.3801, Train Acc: 0.8765\n",
      "  Val Loss: 0.2799, Val Acc: 0.9224\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.3423, Train Acc: 0.8900\n",
      "  Val Loss: 0.2547, Val Acc: 0.9311\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.3302, Train Acc: 0.8924\n",
      "  Val Loss: 0.2503, Val Acc: 0.9284\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.3353, Train Acc: 0.8880\n",
      "  Val Loss: 0.2619, Val Acc: 0.9220\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the specified device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # Disable gradient calculation for validation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation data\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update total and correct predictions\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e447ee3",
   "metadata": {},
   "source": [
    "Tuning the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c44055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae15b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01906f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddfa805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = WasteDataset(\n",
    "    data_dir='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = WasteDataset(\n",
    "    data_dir='data/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee93fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0df1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9ed9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01):\n",
    "    model = WasteClassifierMobileNet(num_classes=6)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da20ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Learning Rate: 0.0001 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.5959, Train Acc: 0.3941\n",
      "  Val Loss: 1.3968, Val Acc: 0.5841\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.2892, Train Acc: 0.6284\n",
      "  Val Loss: 1.1455, Val Acc: 0.6996\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.0949, Train Acc: 0.7016\n",
      "  Val Loss: 0.9849, Val Acc: 0.7499\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.9695, Train Acc: 0.7396\n",
      "  Val Loss: 0.8759, Val Acc: 0.7772\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.8790, Train Acc: 0.7590\n",
      "  Val Loss: 0.7984, Val Acc: 0.7831\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.8069, Train Acc: 0.7776\n",
      "  Val Loss: 0.7430, Val Acc: 0.8065\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.7571, Train Acc: 0.7875\n",
      "  Val Loss: 0.6901, Val Acc: 0.8108\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.7163, Train Acc: 0.7982\n",
      "  Val Loss: 0.6562, Val Acc: 0.8275\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.6741, Train Acc: 0.8061\n",
      "  Val Loss: 0.6168, Val Acc: 0.8346\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.6529, Train Acc: 0.8156\n",
      "  Val Loss: 0.5924, Val Acc: 0.8445\n",
      "\n",
      "=== Learning Rate: 0.001 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.0218, Train Acc: 0.6466\n",
      "  Val Loss: 0.6130, Val Acc: 0.8104\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.6059, Train Acc: 0.8061\n",
      "  Val Loss: 0.4667, Val Acc: 0.8666\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.5196, Train Acc: 0.8283\n",
      "  Val Loss: 0.4126, Val Acc: 0.8702\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.4345, Train Acc: 0.8567\n",
      "  Val Loss: 0.3459, Val Acc: 0.9015\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.4065, Train Acc: 0.8738\n",
      "  Val Loss: 0.3210, Val Acc: 0.9046\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.3935, Train Acc: 0.8686\n",
      "  Val Loss: 0.3206, Val Acc: 0.9027\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.3711, Train Acc: 0.8773\n",
      "  Val Loss: 0.3022, Val Acc: 0.9038\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.3593, Train Acc: 0.8797\n",
      "  Val Loss: 0.2815, Val Acc: 0.9125\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.3324, Train Acc: 0.8955\n",
      "  Val Loss: 0.2500, Val Acc: 0.9315\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.3105, Train Acc: 0.8951\n",
      "  Val Loss: 0.2398, Val Acc: 0.9292\n",
      "\n",
      "=== Learning Rate: 0.01 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.1846, Train Acc: 0.6514\n",
      "  Val Loss: 0.3903, Val Acc: 0.8694\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.5731, Train Acc: 0.8164\n",
      "  Val Loss: 0.4383, Val Acc: 0.8528\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.4266, Train Acc: 0.8508\n",
      "  Val Loss: 0.5767, Val Acc: 0.8101\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5116, Train Acc: 0.8465\n",
      "  Val Loss: 0.2838, Val Acc: 0.9007\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.3865, Train Acc: 0.8753\n",
      "  Val Loss: 0.1534, Val Acc: 0.9446\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.4236, Train Acc: 0.8666\n",
      "  Val Loss: 0.1944, Val Acc: 0.9359\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.4290, Train Acc: 0.8706\n",
      "  Val Loss: 0.1784, Val Acc: 0.9414\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.3927, Train Acc: 0.8789\n",
      "  Val Loss: 0.4018, Val Acc: 0.8793\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.5436, Train Acc: 0.8528\n",
      "  Val Loss: 0.1484, Val Acc: 0.9517\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.3172, Train Acc: 0.8920\n",
      "  Val Loss: 0.1902, Val Acc: 0.9359\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "num_epochs = 10\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'\\n=== Learning Rate: {lr} ===')\n",
    "    model, optimizer = make_model(learning_rate=lr)\n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3690f20",
   "metadata": {},
   "source": [
    "Best:  \n",
    "=== Learning Rate: 0.01 ===  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3e1f8",
   "metadata": {},
   "source": [
    "Adding inner layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34209a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8704a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53221bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a68399e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = WasteDataset(\n",
    "    data_dir='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = WasteDataset(\n",
    "    data_dir='data/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b300e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "        \n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze everything\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last block (features[18] is last inverted residual block)\n",
    "        for param in self.base_model.features[18].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17928959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4248ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Checkpoint the model if validation accuracy improved\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'model/mobilenet_v2_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26b69567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner=100):\n",
    "    model = WasteClassifierMobileNet(\n",
    "        num_classes=6,\n",
    "        size_inner=size_inner\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03ba6bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Size inner: 10 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.0438, Train Acc: 0.6003\n",
      "  Val Loss: 0.7973, Val Acc: 0.7064\n",
      "Checkpoint saved: mobilenet_v2_01_0.706.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.7672, Train Acc: 0.7317\n",
      "  Val Loss: 0.5686, Val Acc: 0.8239\n",
      "Checkpoint saved: mobilenet_v2_02_0.824.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.6466, Train Acc: 0.7922\n",
      "  Val Loss: 0.4806, Val Acc: 0.8544\n",
      "Checkpoint saved: mobilenet_v2_03_0.854.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5538, Train Acc: 0.8128\n",
      "  Val Loss: 0.4035, Val Acc: 0.8623\n",
      "Checkpoint saved: mobilenet_v2_04_0.862.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.5145, Train Acc: 0.8203\n",
      "  Val Loss: 0.3507, Val Acc: 0.8773\n",
      "Checkpoint saved: mobilenet_v2_05_0.877.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.4611, Train Acc: 0.8472\n",
      "  Val Loss: 0.3627, Val Acc: 0.8738\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.4090, Train Acc: 0.8607\n",
      "  Val Loss: 0.2880, Val Acc: 0.9121\n",
      "Checkpoint saved: mobilenet_v2_07_0.912.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.3989, Train Acc: 0.8694\n",
      "  Val Loss: 0.2206, Val Acc: 0.9410\n",
      "Checkpoint saved: mobilenet_v2_08_0.941.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.2875, Train Acc: 0.9102\n",
      "  Val Loss: 0.1914, Val Acc: 0.9434\n",
      "Checkpoint saved: mobilenet_v2_09_0.943.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.2809, Train Acc: 0.9133\n",
      "  Val Loss: 0.2259, Val Acc: 0.9276\n",
      "\n",
      "=== Size inner: 100 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 0.9377, Train Acc: 0.7052\n",
      "  Val Loss: 0.4131, Val Acc: 0.8635\n",
      "Checkpoint saved: mobilenet_v2_01_0.863.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.4892, Train Acc: 0.8358\n",
      "  Val Loss: 0.3604, Val Acc: 0.8817\n",
      "Checkpoint saved: mobilenet_v2_02_0.882.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.4314, Train Acc: 0.8579\n",
      "  Val Loss: 0.3500, Val Acc: 0.8710\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.3764, Train Acc: 0.8726\n",
      "  Val Loss: 0.2618, Val Acc: 0.9118\n",
      "Checkpoint saved: mobilenet_v2_04_0.912.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.3017, Train Acc: 0.8884\n",
      "  Val Loss: 0.1583, Val Acc: 0.9513\n",
      "Checkpoint saved: mobilenet_v2_05_0.951.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.2505, Train Acc: 0.9137\n",
      "  Val Loss: 0.1285, Val Acc: 0.9620\n",
      "Checkpoint saved: mobilenet_v2_06_0.962.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.2129, Train Acc: 0.9280\n",
      "  Val Loss: 0.1595, Val Acc: 0.9442\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.1840, Train Acc: 0.9418\n",
      "  Val Loss: 0.1083, Val Acc: 0.9660\n",
      "Checkpoint saved: mobilenet_v2_08_0.966.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.2278, Train Acc: 0.9216\n",
      "  Val Loss: 0.0723, Val Acc: 0.9802\n",
      "Checkpoint saved: mobilenet_v2_09_0.980.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.1514, Train Acc: 0.9478\n",
      "  Val Loss: 0.0739, Val Acc: 0.9735\n",
      "\n",
      "=== Size inner: 500 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.3681, Train Acc: 0.6842\n",
      "  Val Loss: 0.4892, Val Acc: 0.8401\n",
      "Checkpoint saved: mobilenet_v2_01_0.840.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.5425, Train Acc: 0.8077\n",
      "  Val Loss: 0.3603, Val Acc: 0.8746\n",
      "Checkpoint saved: mobilenet_v2_02_0.875.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.4331, Train Acc: 0.8567\n",
      "  Val Loss: 0.2557, Val Acc: 0.9161\n",
      "Checkpoint saved: mobilenet_v2_03_0.916.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.3800, Train Acc: 0.8746\n",
      "  Val Loss: 0.3710, Val Acc: 0.8797\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.3395, Train Acc: 0.8844\n",
      "  Val Loss: 0.2047, Val Acc: 0.9304\n",
      "Checkpoint saved: mobilenet_v2_05_0.930.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.2643, Train Acc: 0.9121\n",
      "  Val Loss: 0.1528, Val Acc: 0.9470\n",
      "Checkpoint saved: mobilenet_v2_06_0.947.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.2792, Train Acc: 0.9110\n",
      "  Val Loss: 0.1190, Val Acc: 0.9588\n",
      "Checkpoint saved: mobilenet_v2_07_0.959.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.2076, Train Acc: 0.9335\n",
      "  Val Loss: 0.1051, Val Acc: 0.9636\n",
      "Checkpoint saved: mobilenet_v2_08_0.964.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.2098, Train Acc: 0.9276\n",
      "  Val Loss: 0.0898, Val Acc: 0.9695\n",
      "Checkpoint saved: mobilenet_v2_09_0.970.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.1738, Train Acc: 0.9402\n",
      "  Val Loss: 0.1007, Val Acc: 0.9620\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "size_inners = [10, 100, 500]\n",
    "num_epochs = 10\n",
    "\n",
    "for size in size_inners:\n",
    "    print(f'\\n=== Size inner: {size} ===')\n",
    "    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size)\n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbcb1d",
   "metadata": {},
   "source": [
    "Best:   \n",
    "=== Size inner: 100 ===  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85920f",
   "metadata": {},
   "source": [
    "Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2664881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c031ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fabac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "503a5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = WasteDataset(\n",
    "    data_dir='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = WasteDataset(\n",
    "    data_dir='data/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9d2c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "        \n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(droprate)  # Add dropout\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf8654d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b095c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Checkpoint the model if validation accuracy improved\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'model/mobilenet_v3_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ef986e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        learning_rate=0.01,\n",
    "        size_inner=100,\n",
    "        droprate=0.2\n",
    "):\n",
    "    model = WasteClassifierMobileNet(\n",
    "        num_classes=6,\n",
    "        size_inner=size_inner,\n",
    "        droprate=droprate\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed030a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Drop rate: 0.0 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.4191, Train Acc: 0.6035\n",
      "  Val Loss: 0.5339, Val Acc: 0.8180\n",
      "Checkpoint saved: mobilenet_v3_01_0.818.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.6199, Train Acc: 0.7768\n",
      "  Val Loss: 0.4657, Val Acc: 0.8417\n",
      "Checkpoint saved: mobilenet_v3_02_0.842.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.5825, Train Acc: 0.7855\n",
      "  Val Loss: 0.3666, Val Acc: 0.8746\n",
      "Checkpoint saved: mobilenet_v3_03_0.875.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5132, Train Acc: 0.8116\n",
      "  Val Loss: 0.3601, Val Acc: 0.8773\n",
      "Checkpoint saved: mobilenet_v3_04_0.877.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.4574, Train Acc: 0.8425\n",
      "  Val Loss: 0.2847, Val Acc: 0.9030\n",
      "Checkpoint saved: mobilenet_v3_05_0.903.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.5070, Train Acc: 0.8223\n",
      "  Val Loss: 0.4019, Val Acc: 0.8556\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.4028, Train Acc: 0.8587\n",
      "  Val Loss: 0.3127, Val Acc: 0.9011\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.4166, Train Acc: 0.8488\n",
      "  Val Loss: 0.3263, Val Acc: 0.8765\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.4134, Train Acc: 0.8496\n",
      "  Val Loss: 0.2547, Val Acc: 0.9177\n",
      "Checkpoint saved: mobilenet_v3_09_0.918.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.3706, Train Acc: 0.8734\n",
      "  Val Loss: 0.2492, Val Acc: 0.9153\n",
      "\n",
      "=== Drop rate: 0.2 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.3666, Train Acc: 0.5208\n",
      "  Val Loss: 0.6876, Val Acc: 0.7499\n",
      "Checkpoint saved: mobilenet_v3_01_0.750.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.8391, Train Acc: 0.6874\n",
      "  Val Loss: 0.6411, Val Acc: 0.7416\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.7466, Train Acc: 0.7210\n",
      "  Val Loss: 0.4599, Val Acc: 0.8231\n",
      "Checkpoint saved: mobilenet_v3_03_0.823.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.6828, Train Acc: 0.7511\n",
      "  Val Loss: 0.4793, Val Acc: 0.8330\n",
      "Checkpoint saved: mobilenet_v3_04_0.833.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.7076, Train Acc: 0.7313\n",
      "  Val Loss: 0.4623, Val Acc: 0.8346\n",
      "Checkpoint saved: mobilenet_v3_05_0.835.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.6329, Train Acc: 0.7618\n",
      "  Val Loss: 0.5571, Val Acc: 0.7950\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.7603, Train Acc: 0.7111\n",
      "  Val Loss: 0.5020, Val Acc: 0.8251\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.7024, Train Acc: 0.7452\n",
      "  Val Loss: 0.3573, Val Acc: 0.8698\n",
      "Checkpoint saved: mobilenet_v3_08_0.870.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.6893, Train Acc: 0.7527\n",
      "  Val Loss: 0.3875, Val Acc: 0.8710\n",
      "Checkpoint saved: mobilenet_v3_09_0.871.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.6676, Train Acc: 0.7590\n",
      "  Val Loss: 0.3982, Val Acc: 0.8500\n",
      "\n",
      "=== Drop rate: 0.5 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.5962, Train Acc: 0.4357\n",
      "  Val Loss: 0.8543, Val Acc: 0.7277\n",
      "Checkpoint saved: mobilenet_v3_01_0.728.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.1646, Train Acc: 0.5188\n",
      "  Val Loss: 0.7730, Val Acc: 0.7748\n",
      "Checkpoint saved: mobilenet_v3_02_0.775.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.1123, Train Acc: 0.5512\n",
      "  Val Loss: 0.7801, Val Acc: 0.7467\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.0660, Train Acc: 0.5730\n",
      "  Val Loss: 0.6552, Val Acc: 0.8053\n",
      "Checkpoint saved: mobilenet_v3_04_0.805.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 1.0301, Train Acc: 0.5956\n",
      "  Val Loss: 0.6244, Val Acc: 0.7946\n",
      "Epoch 6/10\n",
      "  Train Loss: 1.0684, Train Acc: 0.5600\n",
      "  Val Loss: 0.5971, Val Acc: 0.8259\n",
      "Checkpoint saved: mobilenet_v3_06_0.826.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 1.0608, Train Acc: 0.5861\n",
      "  Val Loss: 0.6142, Val Acc: 0.8089\n",
      "Epoch 8/10\n",
      "  Train Loss: 1.0749, Train Acc: 0.5714\n",
      "  Val Loss: 0.6742, Val Acc: 0.7729\n",
      "Epoch 9/10\n",
      "  Train Loss: 1.0214, Train Acc: 0.5928\n",
      "  Val Loss: 0.5688, Val Acc: 0.8199\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.9923, Train Acc: 0.6082\n",
      "  Val Loss: 0.5054, Val Acc: 0.8306\n",
      "Checkpoint saved: mobilenet_v3_10_0.831.pth\n",
      "\n",
      "=== Drop rate: 0.8 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.9250, Train Acc: 0.2236\n",
      "  Val Loss: 1.7333, Val Acc: 0.2355\n",
      "Checkpoint saved: mobilenet_v3_01_0.235.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.7317, Train Acc: 0.2382\n",
      "  Val Loss: 1.7248, Val Acc: 0.2355\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.7277, Train Acc: 0.2359\n",
      "  Val Loss: 1.7240, Val Acc: 0.2355\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.7248, Train Acc: 0.2351\n",
      "  Val Loss: 1.7236, Val Acc: 0.2351\n",
      "Epoch 5/10\n",
      "  Train Loss: 1.7239, Train Acc: 0.2351\n",
      "  Val Loss: 1.7237, Val Acc: 0.2351\n",
      "Epoch 6/10\n",
      "  Train Loss: 1.7246, Train Acc: 0.2351\n",
      "  Val Loss: 1.7238, Val Acc: 0.2355\n",
      "Epoch 7/10\n",
      "  Train Loss: 1.7239, Train Acc: 0.2355\n",
      "  Val Loss: 1.5852, Val Acc: 0.2576\n",
      "Checkpoint saved: mobilenet_v3_07_0.258.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 1.7236, Train Acc: 0.2355\n",
      "  Val Loss: 1.7236, Val Acc: 0.2351\n",
      "Epoch 9/10\n",
      "  Train Loss: 1.7239, Train Acc: 0.2351\n",
      "  Val Loss: 1.7235, Val Acc: 0.2351\n",
      "Epoch 10/10\n",
      "  Train Loss: 1.7248, Train Acc: 0.2351\n",
      "  Val Loss: 1.7238, Val Acc: 0.2351\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "size_inner = 100\n",
    "droprate = [0.0, 0.2, 0.5, 0.8]\n",
    "num_epochs = 10\n",
    "\n",
    "for dr in droprate:\n",
    "    print(f'\\n=== Drop rate: {dr} ===')\n",
    "    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner, droprate=dr)\n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758476a9",
   "metadata": {},
   "source": [
    "Dropout was not considered for the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc9861",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b53a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e306fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7c92e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9b29444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = WasteDataset(\n",
    "    data_dir='data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = WasteDataset(\n",
    "    data_dir='data/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dc5f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "        \n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last block (features[18] is last inverted residual block)\n",
    "        for param in self.base_model.features[18].parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "898e1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee15dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Checkpoint the model if validation accuracy improved\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'model/mobilenet_v4_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa784074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        learning_rate=0.01,\n",
    "        size_inner=100,\n",
    "):\n",
    "    model = WasteClassifierMobileNet(\n",
    "        num_classes=6,\n",
    "        size_inner=size_inner,\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c3ed651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Leaning rate: 0.01, size_inner: 100 ===\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.9012, Train Acc: 0.6977\n",
      "  Val Loss: 0.4433, Val Acc: 0.8457\n",
      "Checkpoint saved: model/mobilenet_v4_01_0.846.pth\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.5011, Train Acc: 0.8184\n",
      "  Val Loss: 0.3253, Val Acc: 0.8813\n",
      "Checkpoint saved: model/mobilenet_v4_02_0.881.pth\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.4429, Train Acc: 0.8429\n",
      "  Val Loss: 0.2757, Val Acc: 0.9007\n",
      "Checkpoint saved: model/mobilenet_v4_03_0.901.pth\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.3605, Train Acc: 0.8765\n",
      "  Val Loss: 0.2135, Val Acc: 0.9272\n",
      "Checkpoint saved: model/mobilenet_v4_04_0.927.pth\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.3139, Train Acc: 0.8896\n",
      "  Val Loss: 0.1590, Val Acc: 0.9478\n",
      "Checkpoint saved: model/mobilenet_v4_05_0.948.pth\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.2330, Train Acc: 0.9205\n",
      "  Val Loss: 0.1322, Val Acc: 0.9573\n",
      "Checkpoint saved: model/mobilenet_v4_06_0.957.pth\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.2294, Train Acc: 0.9161\n",
      "  Val Loss: 0.1161, Val Acc: 0.9624\n",
      "Checkpoint saved: model/mobilenet_v4_07_0.962.pth\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.2012, Train Acc: 0.9288\n",
      "  Val Loss: 0.1337, Val Acc: 0.9517\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.1920, Train Acc: 0.9379\n",
      "  Val Loss: 0.0687, Val Acc: 0.9790\n",
      "Checkpoint saved: model/mobilenet_v4_09_0.979.pth\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.1731, Train Acc: 0.9442\n",
      "  Val Loss: 0.1036, Val Acc: 0.9656\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.1472, Train Acc: 0.9482\n",
      "  Val Loss: 0.0473, Val Acc: 0.9826\n",
      "Checkpoint saved: model/mobilenet_v4_11_0.983.pth\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.1345, Train Acc: 0.9525\n",
      "  Val Loss: 0.1143, Val Acc: 0.9632\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.1129, Train Acc: 0.9577\n",
      "  Val Loss: 0.0481, Val Acc: 0.9822\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.1015, Train Acc: 0.9668\n",
      "  Val Loss: 0.0444, Val Acc: 0.9854\n",
      "Checkpoint saved: model/mobilenet_v4_14_0.985.pth\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0829, Train Acc: 0.9699\n",
      "  Val Loss: 0.0462, Val Acc: 0.9818\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0706, Train Acc: 0.9711\n",
      "  Val Loss: 0.0341, Val Acc: 0.9885\n",
      "Checkpoint saved: model/mobilenet_v4_16_0.989.pth\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.1180, Train Acc: 0.9683\n",
      "  Val Loss: 0.0432, Val Acc: 0.9834\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0788, Train Acc: 0.9755\n",
      "  Val Loss: 0.0304, Val Acc: 0.9873\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0662, Train Acc: 0.9759\n",
      "  Val Loss: 0.0260, Val Acc: 0.9937\n",
      "Checkpoint saved: model/mobilenet_v4_19_0.994.pth\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0590, Train Acc: 0.9794\n",
      "  Val Loss: 0.0466, Val Acc: 0.9838\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0888, Train Acc: 0.9707\n",
      "  Val Loss: 0.0458, Val Acc: 0.9850\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0719, Train Acc: 0.9774\n",
      "  Val Loss: 0.0270, Val Acc: 0.9909\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0670, Train Acc: 0.9763\n",
      "  Val Loss: 0.0392, Val Acc: 0.9889\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0684, Train Acc: 0.9767\n",
      "  Val Loss: 0.0237, Val Acc: 0.9913\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0618, Train Acc: 0.9790\n",
      "  Val Loss: 0.0221, Val Acc: 0.9925\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0625, Train Acc: 0.9774\n",
      "  Val Loss: 0.0271, Val Acc: 0.9917\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0887, Train Acc: 0.9743\n",
      "  Val Loss: 0.0259, Val Acc: 0.9921\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0390, Train Acc: 0.9865\n",
      "  Val Loss: 0.0100, Val Acc: 0.9968\n",
      "Checkpoint saved: model/mobilenet_v4_28_0.997.pth\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0586, Train Acc: 0.9822\n",
      "  Val Loss: 0.0086, Val Acc: 0.9972\n",
      "Checkpoint saved: model/mobilenet_v4_29_0.997.pth\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0392, Train Acc: 0.9873\n",
      "  Val Loss: 0.0444, Val Acc: 0.9846\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0773, Train Acc: 0.9743\n",
      "  Val Loss: 0.0608, Val Acc: 0.9838\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0597, Train Acc: 0.9838\n",
      "  Val Loss: 0.0147, Val Acc: 0.9960\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0353, Train Acc: 0.9861\n",
      "  Val Loss: 0.0096, Val Acc: 0.9964\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0399, Train Acc: 0.9858\n",
      "  Val Loss: 0.0192, Val Acc: 0.9945\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0361, Train Acc: 0.9893\n",
      "  Val Loss: 0.0092, Val Acc: 0.9964\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0515, Train Acc: 0.9822\n",
      "  Val Loss: 0.0212, Val Acc: 0.9917\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0430, Train Acc: 0.9846\n",
      "  Val Loss: 0.0105, Val Acc: 0.9953\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0367, Train Acc: 0.9893\n",
      "  Val Loss: 0.0192, Val Acc: 0.9937\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0306, Train Acc: 0.9897\n",
      "  Val Loss: 0.0072, Val Acc: 0.9984\n",
      "Checkpoint saved: model/mobilenet_v4_39_0.998.pth\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0437, Train Acc: 0.9850\n",
      "  Val Loss: 0.0135, Val Acc: 0.9949\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0404, Train Acc: 0.9885\n",
      "  Val Loss: 0.0148, Val Acc: 0.9941\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0417, Train Acc: 0.9858\n",
      "  Val Loss: 0.0072, Val Acc: 0.9980\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0491, Train Acc: 0.9834\n",
      "  Val Loss: 0.0162, Val Acc: 0.9953\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0285, Train Acc: 0.9885\n",
      "  Val Loss: 0.0327, Val Acc: 0.9905\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0435, Train Acc: 0.9877\n",
      "  Val Loss: 0.0166, Val Acc: 0.9956\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0483, Train Acc: 0.9842\n",
      "  Val Loss: 0.0107, Val Acc: 0.9960\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0344, Train Acc: 0.9897\n",
      "  Val Loss: 0.0116, Val Acc: 0.9953\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0331, Train Acc: 0.9861\n",
      "  Val Loss: 0.0164, Val Acc: 0.9929\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0330, Train Acc: 0.9897\n",
      "  Val Loss: 0.0222, Val Acc: 0.9925\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0173, Train Acc: 0.9956\n",
      "  Val Loss: 0.0096, Val Acc: 0.9960\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "size_inner = 100\n",
    "num_epochs = 50\n",
    "\n",
    "print(f'\\n=== Leaning rate: {learning_rate}, size_inner: {size_inner} ===')\n",
    "model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner)\n",
    "train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a3dd3",
   "metadata": {},
   "source": [
    "Using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e57d6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14345d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, num_classes=6):\n",
    "        super(WasteClassifierMobileNet, self).__init__()\n",
    "        \n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last block (features[18] is last inverted residual block)\n",
    "        for param in self.base_model.features[18].parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58b54c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_accuracy(filename):\n",
    "    \"\"\"\n",
    "    Extracts the accuracy value from filenames like:\n",
    "    mobilenet_v3A_06_0.845.pth\n",
    "    \"\"\"\n",
    "    match = re.search(r\"_([0-9]*\\.[0-9]+)\\.pth$\", filename)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return -1  # If something goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64558116",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d783dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: model/mobilenet_v4_39_0.998.pth\n"
     ]
    }
   ],
   "source": [
    "# Find best checkpoint\n",
    "list_of_files = glob.glob('model/mobilenet_v*.pth')\n",
    "best_model_file = [x for x in list_of_files if str(max([extract_accuracy(x) for x in list_of_files])) in x][0]\n",
    "print(f\"Loading model from: {best_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cad2f45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WasteClassifierMobileNet(\n",
       "  (base_model): MobileNetV2(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (18): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (inner): Linear(in_features=1280, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (output_layer): Linear(in_features=100, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture used during training\n",
    "trained_size_inner = 100      #  change to whatever you trained with\n",
    "num_classes        = 6\n",
    "\n",
    "# Load model\n",
    "model = WasteClassifierMobileNet(\n",
    "    size_inner=trained_size_inner, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_file))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273c92f",
   "metadata": {},
   "source": [
    "Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "350de2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_image_helper import create_preprocessor\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_pytorch_style(X):\n",
    "    # X: shape (1, 224, 224, 3), dtype=float32, values in [0, 255]\n",
    "    X = X / 255.0\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    \n",
    "    # Convert NHWC  NCHW (batch, height, width, channels  batch, channels, height, width)\n",
    "    X = X.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    # Normalize\n",
    "    X = (X - mean) / std\n",
    "    \n",
    "    return X.astype(np.float32)\n",
    "\n",
    "preprocessor = create_preprocessor(preprocess_pytorch_style, target_size=(224, 224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ce353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying cardboard238.jpg from data/test/cardboard folder\n",
      "File: cardboard238.jpg Clasified as: {'cardboard': 16.32851791381836, 'paper': -6.687436580657959, 'metal': -7.029561996459961, 'glass': -7.788281440734863, 'trash': -9.7933988571167, 'plastic': -13.322946548461914}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Classification from random image in data/test\n",
    "path = 'data/test'\n",
    "classes = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "folder = random.choice(classes)\n",
    "files = os.listdir(f'{path}/{folder}')\n",
    "file = random.choice(files)\n",
    "print(f'Classifying {file} from {path}/{folder} folder')\n",
    "\n",
    "X = preprocessor.from_path(f'{path}/{folder}/{file}')\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X).cpu().numpy()[0]\n",
    "\n",
    "result = dict(zip(classes, pred.tolist()))\n",
    "sorted_result = dict(sorted(result.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f'File: {file} Clasified as: {sorted_result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "671a1642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 10.506720542907715, 'paper': -3.291609764099121, 'glass': -5.369290828704834, 'trash': -5.617626190185547, 'metal': -5.633875370025635, 'plastic': -8.832342147827148}\n"
     ]
    }
   ],
   "source": [
    "# Classification from an url\n",
    "\n",
    "url = 'https://storage.googleapis.com/kagglesdsdata/datasets/9083965/14238273/Garbage%20classification/test/cardboard/cardboard112.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20260119%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20260119T094428Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=6efdbb784d786342ee5e566d4562048552db1f0e866da3178949c3064f5cad5a6bb69da0ec9bc5e2a07518269d78ca32d02c3543f76dbc101bd87f0ecbefc6f4e76fa83ca75f325c21061b967902e83a3f273917ed7ac901768d15f2a6118d8817421b8c24d33c7363b161bb8df86b697fc3f3572e813f771561b1fe0da62d36da59e7dad2a4d469f7bcaea3998758ad43530360047a3186bfae2dc04a2ccd6079b2639752f8d1e3152c952b19858e0b34ffbaa0c0bd9828ca2b55dccd44b1fc11adfa72e351acac974bad8d13a51ace5e89c6d43247960041a067cf048d13ae0507188c25b39bc9bfa3fac59ade3383782374cb6d58ae53140361ce96256583'\n",
    "classes = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "\n",
    "X = preprocessor.from_url(url)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X).cpu().numpy()[0]\n",
    "\n",
    "result = dict(zip(classes, pred.tolist()))\n",
    "sorted_result = dict(sorted(result.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c024a4",
   "metadata": {},
   "source": [
    "Exporting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31e6ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_600341/1445849475.py:7: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `WasteClassifierMobileNet([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `WasteClassifierMobileNet([...]` with `torch.export.export(..., strict=False)`... \n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \n",
      "Applied 104 of general pattern rewrite rules.\n",
      "Model exported to model/waste_classifier_mobilenet_v4.onnx\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"model/waste_classifier_mobilenet_v4.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    verbose=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8904f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waste-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
